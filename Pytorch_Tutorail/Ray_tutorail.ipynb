{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.函数并行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 13:02:17,279\tINFO services.py:1169 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1_return: 9 5\n",
      "task2_return: 8 5\n",
      "\u001b[2m\u001b[36m(pid=4480)\u001b[0m the input number: 9\n",
      "\u001b[2m\u001b[36m(pid=4478)\u001b[0m the input number: 10\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import ray\n",
    "#关闭ray，不关闭直接初始化ray可能会报错\n",
    "ray.shutdown()\n",
    "#初始化ray\n",
    "ray.init()\n",
    "\n",
    "@ray.remote(num_returns=2)\n",
    "def ParallelFunction(num):\n",
    "    time.sleep(5)\n",
    "    print(\"the input number:\",num)\n",
    "    return num - 1, 5\n",
    "\n",
    "r1,r2 = ParallelFunction.remote(10)\n",
    "r3,r4 = ParallelFunction.remote(9)\n",
    "\n",
    "task_list = [r1,r2,r3,r4]\n",
    "\n",
    "r1,r2,r3,r4 = ray.get(task_list)\n",
    "\n",
    "print(\"task1_return:\",r1,r2)\n",
    "print(\"task2_return:\",r3,r4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.类并行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 13:19:38,587\tINFO services.py:1169 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 10 51 5\n",
      "\u001b[2m\u001b[36m(pid=5472)\u001b[0m 实现分布初始化功能 5\n",
      "\u001b[2m\u001b[36m(pid=5472)\u001b[0m 这是并行类方法信息: 51\n",
      "\u001b[2m\u001b[36m(pid=5472)\u001b[0m 这是并行类初始化函数信息: 4\n",
      "\u001b[2m\u001b[36m(pid=5473)\u001b[0m 实现分布初始化功能 10\n",
      "\u001b[2m\u001b[36m(pid=5473)\u001b[0m 这是并行类方法信息: 101\n",
      "\u001b[2m\u001b[36m(pid=5473)\u001b[0m 这是并行类初始化函数信息: 9\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import ray\n",
    "#关闭ray\n",
    "ray.shutdown()\n",
    "#初始化ray\n",
    "ray.init()\n",
    "\n",
    "@ray.remote\n",
    "class ParallelClass():\n",
    "    # ray类并行无法获取init方法中的返回值\n",
    "    def __init__(self,num):\n",
    "        print(\"实现分布初始化功能\",num)\n",
    "        self.num = num\n",
    "        #return num - 1, num + 1\n",
    "    \n",
    "    @ray.method(num_returns=2)\n",
    "    def ClassMethod(self,num):\n",
    "        num = num+1\n",
    "        print(\"这是并行类方法信息:\",num)\n",
    "        print(\"这是并行类初始化函数信息:\",self.num-1)\n",
    "        return num,self.num\n",
    "    \n",
    "C1  = ParallelClass.remote(10)\n",
    "C2  = ParallelClass.remote(5)\n",
    "\n",
    "r1,r2 = C1.ClassMethod.remote(100)\n",
    "r3,r4 = C2.ClassMethod.remote(50)\n",
    "\n",
    "task = [r1,r2,r3,r4]\n",
    "#获取并行初始化结果\n",
    "r1,r2,r3,r4 = ray.get(task)\n",
    "print(r1,r2,r3,r4)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.给并行类/函数分配计算资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 16:33:28,322\tINFO services.py:1169 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================== GPUfunction验证 ===================================\n",
      "tensor([22, 24, 26], device='cuda:0') tensor([5, 6, 6], device='cuda:0') tensor([12, 14, 16], device='cuda:0') tensor([3, 3, 4], device='cuda:0')\n",
      "=================================== GPUclass验证 ===================================\n",
      "0.06851840019226074 0.0668189525604248\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import ray\n",
    "import time\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "print(35*\"=\",\"GPUfunction验证\",35*\"=\")\n",
    "\n",
    "@ray.remote(num_gpus=0.5,num_returns=2)\n",
    "def GPUfunction(num):\n",
    "    device = torch.device('cuda')\n",
    "    x = torch.tensor([num+1, num+2, num+3], device=device)\n",
    "    time.sleep(2)\n",
    "    return x*2, x/2\n",
    "\n",
    "f1,f2 = GPUfunction.remote(10)\n",
    "f3,f4 = GPUfunction.remote(5)\n",
    "\n",
    "task = [f1,f2,f3,f4]\n",
    "f1,f2,f3,f4 = ray.get(task)\n",
    "print(f1,f2,f3,f4)\n",
    "\n",
    "print(35*\"=\",\"GPUclass验证\",35*\"=\")\n",
    "\n",
    "\"\"\"\n",
    "1.num_gpus控制一个远程类使用多少份GPU资源\n",
    "2.指定cpu资源数量没作用,是不是因为我的服器只有一个CPU,这里的cpu算力划分是按cpu个数划分不是按单个cpu逻辑核心数划分\n",
    "\"\"\"\n",
    "\n",
    "@ray.remote(num_gpus=0.5)\n",
    "class GPUclass():\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    @ray.method(num_returns=1)\n",
    "    def GpuCount(self,num):\n",
    "        count_gpu = torch.tensor([0]).to(self.device)\n",
    "        num_gpu = torch.tensor([1]).to(self.device)\n",
    "        count_cpu = 0\n",
    "        num_cpu = 1\n",
    "        time1 = time.time()\n",
    "        for i in  range(num):\n",
    "            count_gpu = num_gpu + count_gpu\n",
    "            count_cpu = num_cpu + count_cpu\n",
    "        time2 = time.time()\n",
    "        cost = time2 - time1\n",
    "        return cost\n",
    "\n",
    "\n",
    "C1 = GPUclass.remote()\n",
    "C2 = GPUclass.remote()\n",
    "\n",
    "r1 = C1.GpuCount.remote(10000)\n",
    "r2 = C2.GpuCount.remote(10000)\n",
    "\n",
    "task = [r1,r2]\n",
    "\n",
    "r1,r2 = ray.get(task)\n",
    "\n",
    "print(r1,r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.小结\n",
    "1. 当print函数在分布式函数或分布式类中输出的频率过快时，ray会报错\n",
    "2. 并行类的构造方无法获取其返回值，类方法可以获取\n",
    "3. 如果单机并行只有一块ＧＰＵ时,GPU要注意分片，不然ray报错\n",
    "4. 单机分布式指定cpu资源数量没作用,是不是因为我的服器只有一个CPU,这里的cpu算力划分是按cpu个数划分不是按单个cpu逻辑核心数划分\n",
    "5. 在有多卡GPU的单机服务器上设置了gpu数据量反而效果不如不设置默认的效果好，设置num_gpus变成了累赘\n",
    "6. 在单卡机器上不设置num_gpus分片数量,会导致程序无法运行"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
