{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.函数并行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 13:02:17,279\tINFO services.py:1169 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1_return: 9 5\n",
      "task2_return: 8 5\n",
      "\u001b[2m\u001b[36m(pid=4480)\u001b[0m the input number: 9\n",
      "\u001b[2m\u001b[36m(pid=4478)\u001b[0m the input number: 10\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import ray\n",
    "#关闭ray，不关闭直接初始化ray可能会报错\n",
    "ray.shutdown()\n",
    "#初始化ray\n",
    "ray.init()\n",
    "\n",
    "@ray.remote(num_returns=2)\n",
    "def ParallelFunction(num):\n",
    "    time.sleep(5)\n",
    "    print(\"the input number:\",num)\n",
    "    return num - 1, 5\n",
    "\n",
    "r1,r2 = ParallelFunction.remote(10)\n",
    "r3,r4 = ParallelFunction.remote(9)\n",
    "\n",
    "task_list = [r1,r2,r3,r4]\n",
    "\n",
    "r1,r2,r3,r4 = ray.get(task_list)\n",
    "\n",
    "print(\"task1_return:\",r1,r2)\n",
    "print(\"task2_return:\",r3,r4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.类并行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 13:19:38,587\tINFO services.py:1169 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 10 51 5\n",
      "\u001b[2m\u001b[36m(pid=5472)\u001b[0m 实现分布初始化功能 5\n",
      "\u001b[2m\u001b[36m(pid=5472)\u001b[0m 这是并行类方法信息: 51\n",
      "\u001b[2m\u001b[36m(pid=5472)\u001b[0m 这是并行类初始化函数信息: 4\n",
      "\u001b[2m\u001b[36m(pid=5473)\u001b[0m 实现分布初始化功能 10\n",
      "\u001b[2m\u001b[36m(pid=5473)\u001b[0m 这是并行类方法信息: 101\n",
      "\u001b[2m\u001b[36m(pid=5473)\u001b[0m 这是并行类初始化函数信息: 9\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import ray\n",
    "#关闭ray\n",
    "ray.shutdown()\n",
    "#初始化ray\n",
    "ray.init()\n",
    "\n",
    "@ray.remote\n",
    "class ParallelClass():\n",
    "    # ray类并行无法获取init方法中的返回值\n",
    "    def __init__(self,num):\n",
    "        print(\"实现分布初始化功能\",num)\n",
    "        self.num = num\n",
    "        #return num - 1, num + 1\n",
    "    \n",
    "    @ray.method(num_returns=2)\n",
    "    def ClassMethod(self,num):\n",
    "        num = num+1\n",
    "        print(\"这是并行类方法信息:\",num)\n",
    "        print(\"这是并行类初始化函数信息:\",self.num-1)\n",
    "        return num,self.num\n",
    "    \n",
    "C1  = ParallelClass.remote(10)\n",
    "C2  = ParallelClass.remote(5)\n",
    "\n",
    "r1,r2 = C1.ClassMethod.remote(100)\n",
    "r3,r4 = C2.ClassMethod.remote(50)\n",
    "\n",
    "task = [r1,r2,r3,r4]\n",
    "#获取并行初始化结果\n",
    "r1,r2,r3,r4 = ray.get(task)\n",
    "print(r1,r2,r3,r4)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.给并行类/函数分配计算资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 16:29:03,463\tINFO services.py:1169 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================== GPUclass验证 ===================================\n",
      "0.06608915328979492 0.06443953514099121\n",
      "=================================== GPUfunction验证 ===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 16:29:23,642\tWARNING worker.py:1039 -- The actor or task with ID 1e9d04d3b7e4dfb2ffffffffffffffffffffffff01000000 cannot be scheduled right now. It requires {GPU: 0.500000}, {CPU: 1.000000} for placement, but this node only has remaining {14.000000/16.000000 CPU, 14.697266 GiB/14.697266 GiB memory, 0.000000/1.000000 GPU, 5.078125 GiB/5.078125 GiB object_store_memory, 1.000000/1.000000 node:192.168.1.161, 1.000000/1.000000 accelerator_type:RTX}\n",
      ". In total there are 1 pending tasks and 0 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import ray\n",
    "import time\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "print(35*\"=\",\"GPUfunction验证\",35*\"=\")\n",
    "\n",
    "@ray.remote(num_gpus=0.5,num_returns=2,max_calls=1)\n",
    "def GPUfunction(num):\n",
    "    device = torch.device('cuda')\n",
    "    x = torch.tensor([num+1, num+2, num+3], device=device)\n",
    "    time.sleep(5)\n",
    "    return x*2, x/2\n",
    "\n",
    "f1,f2 = GPUfunction.remote(10)\n",
    "f3,f4 = GPUfunction.remote(5)\n",
    "\n",
    "task = [f1,f2,f3,f4]\n",
    "f1,f2,f3,f4 = ray.get(task)\n",
    "print(f1,f2,f3,f4)\n",
    "\n",
    "print(35*\"=\",\"GPUclass验证\",35*\"=\")\n",
    "\n",
    "\"\"\"\n",
    "1.num_gpus控制一个远程类使用多少份GPU资源\n",
    "2.指定cpu资源数量没作用,是不是因为我的服器只有一个CPU,这里的cpu算力划分是按cpu个数划分不是按单个cpu逻辑核心数划分\n",
    "\"\"\"\n",
    "\n",
    "@ray.remote(num_gpus=0.5)\n",
    "class GPUclass():\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    @ray.method(num_returns=1)\n",
    "    def GpuCount(self,num):\n",
    "        count_gpu = torch.tensor([0]).to(self.device)\n",
    "        num_gpu = torch.tensor([1]).to(self.device)\n",
    "        count_cpu = 0\n",
    "        num_cpu = 1\n",
    "        time1 = time.time()\n",
    "        for i in  range(num):\n",
    "            count_gpu = num_gpu + count_gpu\n",
    "            count_cpu = num_cpu + count_cpu\n",
    "        time2 = time.time()\n",
    "        cost = time2 - time1\n",
    "        return cost\n",
    "\n",
    "\n",
    "C1 = GPUclass.remote()\n",
    "C2 = GPUclass.remote()\n",
    "\n",
    "r1 = C1.GpuCount.remote(10000)\n",
    "r2 = C2.GpuCount.remote(10000)\n",
    "\n",
    "task = [r1,r2]\n",
    "\n",
    "r1,r2 = ray.get(task)\n",
    "\n",
    "print(r1,r2)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
