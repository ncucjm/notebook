{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  VGG16 \n",
    "import tensorflow as tf\n",
    "\n",
    "class VGG16(object):\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.00003\n",
    "\n",
    "    def __conv2d(self, name, inputs, filters, kernel_size=(3, 3), activation='relu', padding='same'):\n",
    "\n",
    "        return tf.layers.conv2d(inputs, filters, kernel_size,\n",
    "                                padding=padding,\n",
    "                                activation=tf.nn.relu,\n",
    "                                name=name)\n",
    "    def __conv_block(self, name, inputs, filters, kernel_size=(3, 3), activation='relu', padding='same'):\n",
    "\n",
    "        x = inputs\n",
    "        i = 0\n",
    "        for filter in filters:\n",
    "            x = self.__conv2d('{}_conv{}'.format(name, i), x, filter, kernel_size, activation)\n",
    "            i = i + 1\n",
    "        return tf.layers.max_pooling2d(inputs=x, pool_size=[2, 2], strides=2, padding=padding,\n",
    "                                       data_format='channels_last', name=name + '_pool')\n",
    "\n",
    "    def build(self):\n",
    "        self.phase_train_placeholder = tf.placeholder(tf.bool, name='phase_train')\n",
    "        self.img_batch = tf.placeholder(tf.float32, shape=[None, 224, 224, 3], name='input_img')\n",
    "        self.label_batch = tf.placeholder(tf.float32, shape=[None, 4], name='input_label')\n",
    "        x = self.__conv_block('conv_block1', self.img_batch, [64, 64])\n",
    "        x = self.__conv_block('conv_block2', x, [128, 128])\n",
    "        x = self.__conv_block('conv_block3', x, [256, 256, 256])\n",
    "        x = self.__conv_block('conv_block4', x, [512, 512, 512])\n",
    "        x = self.__conv_block('conv_block5', x, [512, 512, 512])\n",
    "        x = tf.layers.flatten(x)\n",
    "        x = tf.layers.dense(x, units=2048, activation=tf.nn.relu, name='fc1')\n",
    "        x = tf.cond(self.phase_train_placeholder, lambda: tf.layers.dropout(x, 0.5, training=True),\n",
    "                    lambda: tf.layers.dropout(x, 0.5, training=False))\n",
    "        x = tf.layers.dense(x, units=4096, activation=tf.nn.relu,name='fc2')\n",
    "        x = tf.cond(self.phase_train_placeholder, lambda: tf.layers.dropout(x, 0.5, training=True),\n",
    "                    lambda: tf.layers.dropout(x, 0.5, training=False))\n",
    "        self.logits = tf.layers.dense(x, units=4, name=\"logits\")\n",
    "        self.predictions = tf.nn.softmax(self.logits, name=\"predictions\")\n",
    "        self.loss = -tf.reduce_mean(self.label_batch * tf.log(self.predictions), name='loss')\n",
    "        correct_prediction = tf.equal(tf.argmax(self.label_batch, 1), tf.argmax(self.predictions, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        self.opt = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据读取/图片裁剪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 数据读取与预处理\n",
    "import scipy\n",
    "from scipy import io\n",
    "import glob, os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from numpy import array\n",
    "\n",
    "sub_types = ['B-cell', 'T-cell', 'Monocyte', 'Granualocyte']\n",
    "sub_labels = [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n",
    "traval_indiv = ['HS1', 'HS2', 'HS3', 'HS5', 'HS6']\n",
    "test = ['WBC_QPI_022219']\n",
    "\n",
    "def get_tif(file_path, img_size):\n",
    "\n",
    "    phase_img = cv2.imread(file_path, 1)\n",
    "    phase_img = cv2.resize(phase_img, (img_size, img_size))\n",
    "    return phase_img\n",
    "\n",
    "def load_data_path(dirpath, traval_indiv):\n",
    "\n",
    "    img_width, img_height = 224, 224\n",
    "    nb_classes = 4\n",
    "    data_set = []\n",
    "    target_set = []\n",
    "\n",
    "    for dd in traval_indiv:\n",
    "        file_name = os.path.join(dirpath, dd)\n",
    "        for i, cell_type in enumerate(sub_types):\n",
    "            sub_dirs = file_name + '\\\\' + cell_type + '\\\\'\n",
    "            os.chdir(sub_dirs)\n",
    "            all_cell = glob.glob('*.tif')\n",
    "            for file in all_cell:\n",
    "                file_path = sub_dirs + file\n",
    "                data_set.append(file_path)\n",
    "                target_set.append(sub_labels[i])\n",
    "\n",
    "    return data_set, target_set\n",
    "\n",
    "def load_train(sdir,traval_indiv):\n",
    "\n",
    "    data_sets = []\n",
    "    target_sets = []\n",
    "    for d in traval_indiv:\n",
    "        print(d)\n",
    "        dpath = os.path.join(sdir, d)\n",
    "        data_set, target_set = load_data(dpath,sub_types)\n",
    "        data_sets.extend(data_set.tolist())\n",
    "        target_sets.extend(target_set)\n",
    "    return data_sets, target_sets\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dir = 'E:\\PythonProject\\cell_pic'\n",
    "    #load_train(train_dir, traval_indiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "1. 利用 HS1,HS2,HS3,HS5,HS6 的数据进行训练，WBC_QPI_022219的数据进行验证\n",
    "\n",
    "2. epoch = 10, batch_size = 14, learning = 0.00003，每200个batch_size修改learning_rate = 0.95 * learning_rate\n",
    "\n",
    "3. 每50个batch_size保存一次模型文件并使用WBC_QPI_022219的数据集验证一次模型的准确度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-89a8bca42691>:13: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-2-89a8bca42691>:22: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-2-89a8bca42691>:33: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-2-89a8bca42691>:34: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-2-89a8bca42691>:35: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from E:\\mygit\\PythonLearn\\computer_vision\\model_save_file\\20190802-142456/model-93.0-loss0.090.ckpt-1800\n",
      "ecoph 0-0/183,loss:0.03779304772615433,accuracy:1.0\n",
      "ecoph 0-1/183,loss:0.04926013946533203,accuracy:0.9285714030265808\n",
      "ecoph 0-2/183,loss:0.08621936291456223,accuracy:0.7857142686843872\n",
      "ecoph 0-3/183,loss:0.042248401790857315,accuracy:0.9285714030265808\n",
      "ecoph 0-4/183,loss:0.0548042394220829,accuracy:0.9285714030265808\n",
      "ecoph 0-5/183,loss:0.039691660553216934,accuracy:0.9285714030265808\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import math\n",
    "import random\n",
    "traval_indiv = ['HS1', 'HS2', 'HS3', 'HS5', 'HS6']\n",
    "test_indiv = ['WBC_QPI_022219']\n",
    "\n",
    "def train():\n",
    "    img_size = 224\n",
    "    train_dir = 'E:\\PythonProject\\cell_pic'\n",
    "    imgpaths, labels = load_data_path(train_dir, traval_indiv)\n",
    "    train_num = len(imgpaths)\n",
    "    test_imgpaths, test_labels = load_data_path(train_dir, test_indiv)\n",
    "    from datetime import datetime\n",
    "    model_dir = 'E:\\PythonProject\\model_save_file'\n",
    "    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s %(message)s',datefmt='%a, %d %b %Y %H:%M:%S', filename=model_dir + '\\log.txt', filemode='w')\n",
    "    logger = logging.getLogger(__name__)\n",
    "    subdir = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "    dst = os.path.join(model_dir, subdir)\n",
    "    os.mkdir(dst)\n",
    "    model = VGG16()\n",
    "    \n",
    "    # 重新设置图\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        model.build()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver(max_to_keep=20)\n",
    "        saver.restore(sess, 'E:\\\\mygit\\\\PythonLearn\\\\computer_vision\\\\model_save_file\\\\20190802-142456/model-93.0-loss0.090.ckpt-1800')\n",
    "        bacth_size = 14\n",
    "        epoch_size = math.floor(train_num / bacth_size)\n",
    "        indexs = [x for x in range(train_num)]\n",
    "\n",
    "        for j in range(10):\n",
    "            random.shuffle(indexs)\n",
    "            imgpaths = np.array(imgpaths)[indexs]\n",
    "            labels = np.array(labels)[indexs]\n",
    "            tloss = 0\n",
    "            for i in range(int(epoch_size)):\n",
    "                imgpath_batch = imgpaths[i * bacth_size:bacth_size * (i + 1)]\n",
    "                img_batch = [get_tif(imgpath, img_size) for imgpath in imgpath_batch]\n",
    "                img_batch = np.array(img_batch).reshape((bacth_size, img_size, img_size, 3))\n",
    "                label_batch = labels[i * bacth_size:bacth_size * (i + 1)]\n",
    "                label_batch = np.array(label_batch).reshape((bacth_size, 4))\n",
    "                accuracy, loss, _ = sess.run([model.accuracy, model.loss, model.opt],\n",
    "                                             feed_dict={model.phase_train_placeholder: True, model.img_batch: img_batch,\n",
    "                                                        model.label_batch: label_batch})\n",
    "\n",
    "                print('ecoph {}-{}/{},loss:{},accuracy:{}'.format(j, i, epoch_size, loss, accuracy))\n",
    "\n",
    "                tloss = tloss + loss\n",
    "                step = j * epoch_size + i\n",
    "                if step % 50 == 0 and step != 0:\n",
    "                    tloss = tloss / 50\n",
    "                    val_accuracy, val_loss = validate(sess, model, test_imgpaths, test_labels, img_size)\n",
    "                    info = 'ecoph{}-{}-val_accuracy:{}-val_loss:{}-train_loss:{}'.format(j, step, val_accuracy,\n",
    "                                                                                         val_loss, tloss)\n",
    "                    logger.info(info)\n",
    "                    print(info)\n",
    "                    checkpoint_path = os.path.join(dst,\n",
    "                                                   'model-{:.1f}-loss{:.3f}.ckpt'.format(val_accuracy * 100, tloss))\n",
    "\n",
    "                    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=True)\n",
    "\n",
    "                if step % 200 == 0 and step != 0:\n",
    "                    model.learning_rate = model.learning_rate * 0.95\n",
    "                    print('learning_rate:{}'.format(model.learning_rate))\n",
    "                    logger.info('learning_rate:{}'.format(model.learning_rate))\n",
    "\n",
    "\n",
    "def validate(sess, model, imgpaths, labels, img_size):\n",
    "    val_num = len(labels)\n",
    "    bacth_size = 10\n",
    "    epoch_size = int(math.floor(val_num / bacth_size))\n",
    "    total_accu = 0\n",
    "    total_loss = 0\n",
    "    indexs = [x for x in range(val_num)]\n",
    "    random.shuffle(indexs)\n",
    "    imgpaths = np.array(imgpaths)[indexs]\n",
    "    labels = np.array(labels)[indexs]\n",
    "    for i in range(epoch_size):\n",
    "        imgpath_batch = imgpaths[i * bacth_size:bacth_size * (i + 1)]\n",
    "        try:\n",
    "            img_batch = [get_tif(imgpath, img_size) for imgpath in imgpath_batch]\n",
    "            img_batch = np.array(img_batch).reshape((bacth_size, img_size, img_size, 3))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        label_batch = labels[i * bacth_size:bacth_size * (i + 1)]\n",
    "        label_batch = np.array(label_batch).reshape((bacth_size, 4))\n",
    "        accuracy, loss = sess.run([model.accuracy, model.loss],\n",
    "                                  feed_dict={model.phase_train_placeholder: False, model.img_batch: img_batch,\n",
    "                                             model.label_batch: label_batch})\n",
    "        total_accu += accuracy\n",
    "        total_loss += loss\n",
    "\n",
    "    return total_accu / epoch_size, total_loss / epoch_size\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算WBC_QPI_022219数据集三种细胞F1值\n",
    "\n",
    "1. 利用训练过程中在验证集上达到准确度最高的模型为最终模型，对验证集数据进行F1计算\n",
    " \n",
    "2. 分别计算每种细胞的F1值\n",
    "\n",
    "3. F1值计算结果如下:\n",
    "\n",
    "|Model|Test person|Age|Gender|Ethnicity|Weight|Height|Dataset|B|T|M|G|\n",
    "|:----|:----|:---|:----- |-----|--|--|--|--|---|---|---|\n",
    "|ResNet(baseline)|5|37|male|African american|87kg|179cm|WBC_QPI_022219|0/0%|251/93%|202/85%|192/83%|\n",
    "|vgg16|5|37|male|African american|87kg|179cm|WBC_QPI_022219|0/0%|251/93.5%|202/91.6%|192/95.4%|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class TestInterface():\n",
    "    \n",
    "    def __init__(self, sess, hat_model_path):\n",
    "        self.model = VGG16()\n",
    "        self.model.build()\n",
    "        self.sess = sess\n",
    "        self.img_size = 224\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self.sess, hat_model_path)\n",
    "\n",
    "    def validate(self, testdir):\n",
    "        imgsize = self.img_size\n",
    "        test_indiv = ['WBC_QPI_022219']\n",
    "        imgpaths, labels = load_data_path(testdir, test_indiv)\n",
    "        sess = self.sess\n",
    "        model = self.model\n",
    "        imgnum = len(labels)\n",
    "        pre_labels = []\n",
    "        tp = [0, 0, 0, 0]\n",
    "        for i in range(imgnum):\n",
    "            print(i, '/', imgnum)\n",
    "            img = cv2.imread(imgpaths[i], 1)\n",
    "            \n",
    "            # picture resize\n",
    "            if img.shape[0] != imgsize or img.shape[1] != imgsize:\n",
    "                img = cv2.resize(img, (imgsize, imgsize))\n",
    "                \n",
    "            img_batch = np.array([img]).reshape((1, imgsize, imgsize, 3))\n",
    "            label_batch = [labels[i]]\n",
    "            predictions = sess.run(model.predictions,\n",
    "                                   feed_dict={model.phase_train_placeholder: False, model.img_batch: img_batch})\n",
    "            pclassname = int(np.argmax(predictions, 1))\n",
    "            isright = np.equal(np.argmax(label_batch, 1), np.argmax(predictions, 1))\n",
    "            x = [0, 0, 0, 0]\n",
    "            x[pclassname] = 1\n",
    "            pre_labels.append(x)\n",
    "            if isright[0]:\n",
    "                tp[pclassname] += 1\n",
    "\n",
    "        tp_fn = np.sum(np.array(labels), axis=0)\n",
    "        tp_fp = np.sum(np.array(pre_labels), axis=0)\n",
    "        precision = tp / (tp_fp + 1e-6)\n",
    "        recall = tp / (tp_fn + 1e-6)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
    "        print(precision, recall, f1)\n",
    "\n",
    "import logging\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hat_model_path = 'E:\\PythonProject/model_save_file\\20190802-142456/model-93.0-loss0.090.ckpt-1800'\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        test = TestInterface(sess, hat_model_path)\n",
    "        test.validate('E:\\PythonProject\\cell_pic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22. 28.]\n",
      " [49. 64.]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# 新建session with log_device_placement并设置为True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# 运行这个 op.\n",
    "print (sess.run(c))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
