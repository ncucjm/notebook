{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# content\n",
    "- 任务描述\n",
    "- bilstm原理\n",
    "- pytorch BiLstm使用\n",
    "- 构建数据集\n",
    "- BiLSTM 网络拓扑\n",
    "- 模型训练\n",
    "- 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务描述\n",
    "每一个句子中前一个单词预测后一个单词，例如：\n",
    "\n",
    "sentence =  'Lorem ipsum dolor sit amet consectetur adipisicing elit '\n",
    "\n",
    "input: lorem \n",
    "\n",
    "target: ipsum\n",
    "# bilstm原理\n",
    "![deque](pic/bilstm原理.png)\n",
    "# pytorch BiLstm使用\n",
    "       ——————————————————————\n",
    "       rnn = nn.LSTM(input_size=10, hidden_size=20, num_layers=2,bidirectional=True)#(input_size,hidden_size,num_layers)\n",
    "       input = torch.randn(5, 3, 10)#(seq_len, batch, input_size)\n",
    "       h0 = torch.randn(4, 3, 20) #(num_layers,batch,output_size)\n",
    "       c0 = torch.randn(4, 3, 20) #(num_layers,batch,output_size)\n",
    "       output, (hn, cn) = rnn(input, (h0, c0))\n",
    "       ——————————————————————\n",
    "       output.shape #(seq_len, batch, hidden_size*2)\n",
    "       torch.Size([5, 3, 40])\n",
    "       ——————————————————————\n",
    "       hn.shape #(num_layers*2, batch, hidden_size)\n",
    "       torch.Size([4, 3, 20])\n",
    "       ——————————————————————\n",
    "       cn.shape #(num_layers*2, batch, hidden_size)\n",
    "       torch.Size([4, 3, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sentence = (\n",
    "    'Lorem ipsum dolor sit amet consectetur adipisicing elit '\n",
    "    'sed do eiusmod tempor incididunt ut labore et dolore magna '\n",
    "    'aliqua Ut enim ad minim veniam quis nostrud exercitation'\n",
    ")\n",
    "# 构建数据集\n",
    "word_dict = {w: i for i, w in enumerate(list(set(sentence.split())))}\n",
    "number_dict = {i: w for i, w in enumerate(list(set(sentence.split())))}\n",
    "\n",
    "n_class = len(word_dict)\n",
    "max_len = len(sentence.split())\n",
    "\n",
    "'''\n",
    "构造批量数据集\n",
    "1.输入单词使用单词在字典中的index的位置one-hot编码\n",
    "2.目标单词使用词典中单词的index编码\n",
    "3.一个batch有词典维度个one-hot编码组成的矩阵，矩阵的第一行为输入单词的one-hot编码，\n",
    "  其他行为字典第一个词的one-hot编码(凑数？？)\n",
    "'''\n",
    "def make_batch(sentence):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    words = sentence.split()\n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        input = [word_dict[n] for n in words[:(i + 1)]]\n",
    "        input = input + [0] * (max_len - len(input))\n",
    "        target = word_dict[words[i + 1]]\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        '''\n",
    "        # numpy.eye(N, M=None, k=0, dtype=<class 'float'>, order='C')\n",
    "        # Return a 2-D array with ones on the diagonal and zeros elsewhere\n",
    "        '''\n",
    "        target_batch.append(target)\n",
    "    return Variable(torch.Tensor(input_batch)), Variable(torch.LongTensor(target_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM 网络拓扑\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, batch_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "\n",
    "        # 超参\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size,\n",
    "                            hidden_size=self.hidden_size,\n",
    "                            num_layers=self.num_layers,\n",
    "                            bidirectional=True)\n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_size * 2, self.output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        inputs = inputs.transpose(0, 1)\n",
    "        # input : [n_step, batch_size, input_size]\n",
    "\n",
    "        h0 = Variable(torch.zeros(self.num_layers * 2,\n",
    "                                  self.batch_size, self.hidden_size)).to(device)\n",
    "        c0 = Variable(torch.zeros(self.num_layers * 2, \n",
    "                                  self.batch_size, self.hidden_size)).to(device)\n",
    "\n",
    "        out, (_, _) = self.lstm(inputs, (h0, c0))\n",
    "        # 当lstm进行并行batch计算时:inputs的维度 ——> (n_step, batch_size, input_size)\n",
    "        # out: (n_step, batch_size, hidden_size*2) (27, 26, 10)\n",
    "\n",
    "        out = self.fc(out[-1])\n",
    "        # out[-1] (26, 10) (batch_size, hidden*2) 只取最后一个时刻输出的隐层状态\n",
    "        # Decode the hidden state of the last time step\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 cost= 1.000267\n",
      "Epoch: 2000 cost= 0.558328\n",
      "Epoch: 3000 cost= 0.466403\n",
      "Epoch: 4000 cost= 0.417631\n",
      "Epoch: 5000 cost= 0.394575\n",
      "Epoch: 6000 cost= 0.380743\n",
      "Epoch: 7000 cost= 0.374061\n",
      "Epoch: 8000 cost= 0.426788\n",
      "Epoch: 9000 cost= 0.421817\n",
      "Epoch: 10000 cost= 0.369939\n",
      "Lorem ipsum dolor sit amet consectetur adipisicing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua Ut enim ad minim veniam quis nostrud exercitation\n",
      "['dolor', 'ipsum', 'dolor', 'dolor', 'dolor', 'adipisicing', 'elit', 'sed', 'do', 'eiusmod', 'tempor', 'incididunt', 'ut', 'labore', 'et', 'dolore', 'magna', 'aliqua', 'Ut', 'enim', 'ad', 'minim', 'veniam', 'quis', 'quis', 'exercitation']\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "\n",
    "# 获取数据\n",
    "input_batch, target_batch = make_batch(sentence)\n",
    "# input_batch  (26,27,27) (n_step, seq_len, input_size(seq_embedding))\n",
    "# target_batch  (26) (target)\n",
    "\n",
    "# 超参\n",
    "# Hyper-parameters\n",
    "input_size = n_class\n",
    "hidden_size = 5\n",
    "learning_rate = 0.003\n",
    "num_layers = 1  # LSTM 神经网络层数\n",
    "batch_size = 26\n",
    "output_size = 27\n",
    "\n",
    "# 模型初始化\n",
    "model = BiLSTM(input_size, hidden_size, output_size, num_layers, batch_size).to(device)\n",
    "\n",
    "# 确认损失函数/参数优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(10000):\n",
    "\n",
    "    # 清空梯度\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_batch.to(device))\n",
    "    loss = criterion(output, target_batch.to(device))\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print('Epoch:', '%04d'%(epoch+1), 'cost=', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 预测\n",
    "predict = model(input_batch.to(device)).data.max(1, keepdim=True)[1]\n",
    "print(sentence)\n",
    "print([number_dict[n.item()] for n in predict.squeeze()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
